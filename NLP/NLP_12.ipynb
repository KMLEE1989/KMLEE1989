{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading averaged_pereptron_tagger: Package\n",
      "[nltk_data]     'averaged_pereptron_tagger' not found in index\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_pereptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('james', 'NNS'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentene = 'james is working at Disney in London'\n",
    "sentene = pos_tag(word_tokenize(sentene))\n",
    "print(sentene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  james/NNS\n",
      "  is/VBZ\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Disney/NNP)\n",
      "  in/IN\n",
      "  (GPE London/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentene = ne_chunk(sentene)\n",
    "\n",
    "print(sentene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = splits[0].lower()\n",
    "        sentence.append([word, splits[-1]])\n",
    "\n",
    "print(len(tagged_sentences))\n",
    "print(tagged_sentences[0])\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, ner_tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_info = zip(*tagged_sentence)\n",
    "    sentences.append(list(sentence))\n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
    "Y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "Y_train = pad_sequences(Y_train, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=.2, random_state=111)\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes=tag_size)\n",
    "Y_test = to_categorical(Y_test, num_classes=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7188, 70)\n",
      "(7188, 70, 10)\n",
      "(1797, 70)\n",
      "(1797, 70, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 70, 128)           512000    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 70, 512)          788480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 70, 10)           5130      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,305,610\n",
      "Trainable params: 1,305,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "57/57 [==============================] - 9s 61ms/step - loss: 0.2111 - accuracy: 0.8170 - val_loss: 0.1467 - val_accuracy: 0.8347\n",
      "Epoch 2/3\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.1317 - accuracy: 0.8330 - val_loss: 0.1016 - val_accuracy: 0.8446\n",
      "Epoch 3/3\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0887 - accuracy: 0.8688 - val_loss: 0.0748 - val_accuracy: 0.8885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17bbdee1dd8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=3, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 11ms/step - loss: 0.0748 - accuracy: 0.8885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07480484992265701, 0.8884502649307251]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = src_tokenizer.index_word\n",
    "idx2ner = tar_tokenizer.index_word\n",
    "idx2ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  |예측값\n",
      "----------------------------------\n",
      "ballanger        : B-PER   B-ORG\n",
      "beat             : O       O\n",
      "germany          : B-LOC   B-LOC\n",
      "'s               : O       O\n",
      "OOV              : B-PER   O\n",
      "OOV              : I-PER   O\n",
      "2-0              : O       O\n",
      "in               : O       O\n",
      "the              : O       O\n",
      "OOV              : O       O\n",
      "matches          : O       O\n",
      "final            : O       O\n",
      "to               : O       O\n",
      "add              : O       O\n",
      "the              : O       O\n",
      "world            : O       O\n",
      "title            : O       O\n",
      "to               : O       O\n",
      "the              : O       O\n",
      "olympic          : B-MISC  B-MISC\n",
      "gold             : O       O\n",
      "medal            : O       O\n",
      "she              : O       O\n",
      "won              : O       O\n",
      "in               : O       O\n",
      "july             : O       O\n",
      ".                : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 70\n",
    "Y_predicted = model.predict(np.array([X_test[i]]))\n",
    "Y_predicted = np.argmax(Y_predicted, axis=-1)\n",
    "true = np.argmax(Y_test[i], -1)\n",
    "\n",
    "print('{:15}|{:5}|{}'.format('단어','실제값', '예측값'))\n",
    "print('-' * 34)\n",
    "\n",
    "for w, t, pred in zip(X_test[i], true, Y_predicted[0]):\n",
    "    if w != 0:\n",
    "        print('{:17}: {:7} {}'.format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3caf13703c5b1c02abff9fa597e671e1239d1d668b6a345ae62ddadff9d8fc63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
