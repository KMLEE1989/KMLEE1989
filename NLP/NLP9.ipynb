{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "documents[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern,'',d)\n",
    "    return text\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join(w.lower() for w in d.split() if w not in stop_words and len(w)>3)\n",
    "\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.DataFrame({'article':documents})\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.dropna(inplace=True)\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well im not sure about the story nad it did se...\n",
       "1        \\n\\n\\n\\n\\n\\n\\nYeah do you expect people to rea...\n",
       "2        Although I realize that principle is not one o...\n",
       "3        Notwithstanding all the legitimate fuss about ...\n",
       "4        Well I will have to change the scoring on my p...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein an Israeli journalist will be...\n",
       "11310                                                   \\n\n",
       "11311    \\nI agree  Home runs off Clemens are always me...\n",
       "11312    I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                          \\nNo arg...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "news_df['article']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure story seem biased what disagree stat...\n",
       "1        yeah expect people read actually accept hard a...\n",
       "2        although realize principle strongest points wo...\n",
       "3        notwithstanding legitimate fuss proposal much ...\n",
       "4        well change scoring playoff pool unfortunately...\n",
       "                               ...                        \n",
       "11309    danny rubenstein israeli journalist speaking t...\n",
       "11310                                                     \n",
       "11311    agree home runs clemens always memorable kinda...\n",
       "11312    used deskjet orange micros grappler system upd...\n",
       "11313    argument murphy scared hell came last year han...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_stopword)\n",
    "news_df['article']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_news = news_df['article'].apply(tokenize)\n",
    "tokenized_news = tokenized_news.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis=0)\n",
    "print(len(news_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "news_2000 = news_texts[:2000]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_2000)\n",
    "\n",
    "idx2word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "sequences = tokenizer.texts_to_sequences(news_2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29769\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1263, 457, 2, 60, 119, 419, 61, 1374, 22, 69, 3498, 397, 6874, 412, 1173, 373, 2256, 458, 59, 12478, 458, 1900, 3850, 397, 22, 10, 4325, 8749, 177, 303, 136, 154, 664, 12479, 316, 12480, 15, 12481, 4, 790, 12482, 12483, 4917, 8750]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in sequences[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inhuman(8747), mqefgcmbprwaqimakjwqzql(25818) -> 0\n",
      "austria(4324), what(34) -> 1\n",
      "israels(3496), hyperion(15830) -> 0\n",
      "letter(663), nilsson(4273) -> 0\n",
      "government(50), quarrel(7032) -> 0\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2420\n",
      "2420\n"
     ]
    }
   ],
   "source": [
    "print(len(skip_grams))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(seq, vocabulary_size=vocab_size, window_size=10) for seq in sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dot\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec():\n",
    "    target_inputs = Input(shape=(1,), dtype='int32')\n",
    "    target_embedding = Embedding(vocab_size, embed_size)(target_inputs)\n",
    "    \n",
    "    context_inputs = Input(shape=(1,), dtype='int32')\n",
    "    context_embedding = Embedding(vocab_size, embed_size)(context_inputs)\n",
    "    \n",
    "    dot_product = Dot(axes=2)([target_embedding, context_embedding])\n",
    "    dot_product = Reshape((1,), input_shape=(1,1))(dot_product)\n",
    "    output = Activation('sigmoid')(dot_product)\n",
    "    \n",
    "    model = Model(inputs=[target_inputs, context_inputs], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 50)        1488450     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 50)        1488450     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 1)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,976,900\n",
      "Trainable params: 2,976,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 692.9964980855584\n",
      "Epoch: 2 Loss: 665.7974811606109\n",
      "Epoch: 3 Loss: 633.2912906017154\n",
      "Epoch: 4 Loss: 596.6879737945274\n",
      "Epoch: 5 Loss: 557.836424717214\n",
      "Epoch: 6 Loss: 519.0104164713994\n",
      "Epoch: 7 Loss: 481.7197163195815\n",
      "Epoch: 8 Loss: 447.2171741516795\n",
      "Epoch: 9 Loss: 415.839994093054\n",
      "Epoch: 10 Loss: 387.882243967877\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)\n",
    "        \n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('skipgram.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "skipgram = gensim.models.KeyedVectors.load_word2vec_format('skipgram.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indiscriminately', 0.8995411396026611),\n",
       " ('booby', 0.8690037131309509),\n",
       " ('retalliates', 0.8514440655708313),\n",
       " ('ammunitions', 0.8333958983421326),\n",
       " ('traps', 0.8318977355957031),\n",
       " ('sneak', 0.8028048276901245),\n",
       " ('patrols', 0.7218192219734192),\n",
       " ('occupying', 0.7067608833312988),\n",
       " ('financially', 0.6982219815254211),\n",
       " ('israeli', 0.6947730779647827)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.most_similar(positive=['soldier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elmwood', 0.604546308517456),\n",
       " ('communitys', 0.5979717373847961),\n",
       " ('diccon', 0.5949234962463379),\n",
       " ('tzeghagron', 0.5835188031196594),\n",
       " ('argumet', 0.5729897022247314),\n",
       " ('nobody', 0.5566871762275696),\n",
       " ('inadmissable', 0.5526425242424011),\n",
       " ('directed', 0.5524103045463562),\n",
       " ('races', 0.5495313405990601),\n",
       " ('betraying', 0.5352721214294434)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.most_similar(positive=['world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram2cbow(skipgrams):\n",
    "    cbows = []\n",
    "    flag = 0\n",
    "    \n",
    "    for n in skip_grams:\n",
    "        temp1 = []\n",
    "        for t in n:\n",
    "            if flag==1 :\n",
    "                flag = 0\n",
    "                temp1.append(t)\n",
    "            else:\n",
    "                flag = 1\n",
    "                temp2 = []\n",
    "                for x in t:\n",
    "                    temp2.append([x[1], x[0]])\n",
    "                temp1.append(temp2)\n",
    "            cbows.append(temp1)\n",
    "    return cbows\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbows = skipgram2cbow(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reports(1012), report(627) -> 1\n",
      "ruin(12474), sure(64) -> 1\n",
      "yapilamayacak(14474), report(627) -> 0\n",
      "shame(3221), reason(203) -> 1\n",
      "received(387), makes(208) -> 1\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = cbows[0][0], cbows[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "2420\n",
      "2420\n"
     ]
    }
   ],
   "source": [
    "print(len(cbows))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 50)        1488450     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 50)        1488450     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 1, 1)         0           ['embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1)            0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1)            0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,976,900\n",
      "Trainable params: 2,976,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 774.9314250665484\n",
      "Epoch: 2 Loss: 701.2680347991482\n",
      "Epoch: 3 Loss: 642.014219972807\n",
      "Epoch: 4 Loss: 593.6540243061663\n",
      "Epoch: 5 Loss: 553.5183973427438\n",
      "Epoch: 6 Loss: 520.0043771396104\n",
      "Epoch: 7 Loss: 491.66066822369976\n",
      "Epoch: 8 Loss: 467.50972097968975\n",
      "Epoch: 9 Loss: 446.6312315133209\n",
      "Epoch: 10 Loss: 428.4711104764431\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(cbows):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)\n",
    "           \n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('cbow.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "cbow = gensim.models.KeyedVectors.load_word2vec_format('cbow.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rodney', 0.8361040353775024),\n",
       " ('telegraph', 0.8254186511039734),\n",
       " ('angered', 0.8183834552764893),\n",
       " ('retalliates', 0.8049345016479492),\n",
       " ('miraculously', 0.8015758991241455),\n",
       " ('alleen', 0.8004842400550842),\n",
       " ('indiscriminately', 0.7989708185195923),\n",
       " ('mqwuwtwwuwwuwatweuwae', 0.7916582822799683),\n",
       " ('pubamigaincomingimagine', 0.7880551815032959),\n",
       " ('grandson', 0.7875582575798035)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.most_similar(positive=['soldier'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordEmbeddingsKeyedVectors.most_similar of <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001F5E741F518>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.most_similar"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3caf13703c5b1c02abff9fa597e671e1239d1d668b6a345ae62ddadff9d8fc63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
